{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(Optional Task 4) FIT 3183 Week 5 Advanced Lab: Deepfakes I",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd_fRUzxOYuJ"
      },
      "source": [
        "**FIT3183 Malicious AI & Dark Side Security**\n",
        "# Week 5 Advanced Lab: Deepfakes I\n",
        "\n",
        "This notebook demonstrates implementation of Deepfakes with First Order Motion Model for Image Animation and Generative Adversarial Networks for Face Swapping.\n",
        "\n",
        "First, check out the recent Nvidia's Deep Fake of CEO Jensen Huang at GTC [(Behind the Scenes)](https://www.youtube.com/watch?v=f_V30ueEXE4)!\n",
        "\n",
        "There are 3 task this week:\n",
        "\n",
        "1. Task 1: **First Order Motion Model for Image Animation** \n",
        "- Demo for Animation, FaceSwap, Wav2Lip\n",
        "2. Task 2: Part Swap Demo for **Motion Supervised co-part Segmentation**\n",
        "3. (Optional) Advanced Task 3: **DeepFake Lite Training with DeepFaceLab**\n",
        "4. (Optional) Advanced Task 4: **Motion Representations for Articulated Animation**\n",
        "\n",
        "Some Deepfake Video Examples:\n",
        "* [The Mona Lisa and Elon Musk](https://www.youtube.com/watch?v=GkOA0X-A58Q)\n",
        "* [Kim Kardashian and Dr. K](https://www.youtube.com/watch?v=QmdbtEPlOVA)\n",
        "* [Woman and Edward Snowden](https://www.youtube.com/watch?v=wSlC39NYenY)\n",
        "\n",
        "Deepfakes are faked media that mimic another person's likeness. This process is achieved through something called \"deep learning,\" hence the name deepfake.\n",
        "\n",
        "Deepfakes Behind the Scenes:\n",
        "1. Extraction\n",
        "- Face Detection\n",
        "- Face Alignment\n",
        "- Face Segmentation\n",
        "2. Training\n",
        "3. Conversion\n",
        "- Blending\n",
        "- Sharpening\n",
        "\n",
        "*OpenSource:*\n",
        "\n",
        "[DeepFaceLab](https://github.com/iperov/DeepFaceLab) - the leading software for creating deepfakes\n",
        "\n",
        "Paper Reference: https://arxiv.org/abs/2005.05535\n",
        "\n",
        "ðŸ‘‰ *Copy this Colab notebook to your Drive*, read the instruction and fill the missing code.\n",
        "\n",
        "ðŸ‘‰ *Use GPU:* `Runtime > Change runtime type > GPU`.\n",
        "\n",
        "\n",
        "<small>*Prepared by [Yin Yin Low](mailto:yin.low@monash.edu) (Lab Tutor) Aug 2022.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge2HiN1q9-Ed"
      },
      "source": [
        "## Task 4: Motion Representations for Articulated Animation\n",
        "\n",
        "Paper reference: https://arxiv.org/abs/2104.11280\n",
        "\n",
        "**Overview**\n",
        "\n",
        "The region predictor returns heatmaps for each part in the source and the driving images. We then compute principal axes of each heatmap, to transform each region from the source to the driving frame through a whitened reference frame. Region and background transformations are combined by the pixel-wise flow prediction network. The target image is generated by warping the source image in the feature space using the pixel-wise flow, and inpainting newly introduced regions, as indicated by the confidence map.\n",
        "\n",
        "<img src=\"https://snap-research.github.io/articulated-animation/resources/framework.png\">\n",
        "\n",
        "**Example Animation**\n",
        "Here is an example of several images produced by motion representations for articulated animation method. In the first column the driving video is shown. For the remaining columns the top image is animated by using motions extracted from the driving.\n",
        "<img src=\"https://github.com/snap-research/articulated-animation/raw/main/sup-mat/teaser.gif\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEpv8c1d_RID"
      },
      "source": [
        "!rm -rf /content/articulated-animation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qIHfBeH-gWO"
      },
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs install\n",
        "!git clone https://github.com/snap-research/articulated-animation.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwhL27dB-jtU"
      },
      "source": [
        "cd articulated-animation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX1wM983-ulx"
      },
      "source": [
        "### Step 1: Load source image and driving video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYCcCqRM-pIS"
      },
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage.transform import resize\n",
        "from IPython.display import HTML\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "source_image = imageio.imread('sup-mat/source.png')\n",
        "driving_video = imageio.mimread('sup-mat/driving.mp4')\n",
        "\n",
        "source_image = resize(source_image, (384, 384))[..., :3]\n",
        "driving_video = [resize(frame, (384, 384))[..., :3] for frame in driving_video]\n",
        "\n",
        "def display(source, driving, generated=None):\n",
        "    fig = plt.figure(figsize=(8 + 4 * (generated is not None), 6))\n",
        "\n",
        "    ims = []\n",
        "    for i in range(len(driving)):\n",
        "        cols = [source]\n",
        "        cols.append(driving[i])\n",
        "        if generated is not None:\n",
        "            cols.append(generated[i])\n",
        "        im = plt.imshow(np.concatenate(cols, axis=1), animated=True)\n",
        "        plt.axis('off')\n",
        "        ims.append([im])\n",
        "\n",
        "    ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n",
        "    plt.close()\n",
        "    return ani\n",
        "    \n",
        "## YOUR CODE STARTS HERE ##\n",
        "# Display the source image and driving video with HTML function (1 line)\n",
        "\n",
        "## YOUR CODE ENDS HERE ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMCT-Bx2-z7H"
      },
      "source": [
        "### Step 2: Load checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPgFQsBR-yBh"
      },
      "source": [
        "from demo import load_checkpoints\n",
        "\n",
        "generator, region_predictor, avd_network = load_checkpoints(config_path='config/ted384.yaml',\n",
        "                                                            checkpoint_path='checkpoints/ted384.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Lw8TPk-2t1"
      },
      "source": [
        "### Step 3: Perform Animation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRgK2Rvm-2Sp"
      },
      "source": [
        "from demo import make_animation\n",
        "from skimage import img_as_ubyte\n",
        "\n",
        "predictions = make_animation(source_image, driving_video, generator, \n",
        "                             region_predictor, avd_network, animation_mode='avd')\n",
        "\n",
        "#save resulting video\n",
        "imageio.mimsave('generated.mp4', [img_as_ubyte(frame) for frame in predictions])\n",
        "#video can be downloaded from /content folder\n",
        "\n",
        "## YOUR CODE STARTS HERE ##\n",
        "# Display the source image, driving video and predictions with HTML function (1 line)\n",
        "\n",
        "## YOUR CODE ENDS HERE ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0GUhD6e-8Ut"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}